<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta property="og:image" content="https://priml-workshop.github.io/priml2019/img/preview-image.png" />
    <meta property="og:image:alt" content="Privacy in Machine Learning (PriML) NeurIPS 2019 Workshop" />
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Privacy in Machine Learning (NeurIPS 2019 Workshop)" />
    <meta name="twitter:image" content="https://priml-workshop.github.io/priml2019/img/twitter-img.png" />
    <meta name="twitter:image:alt" content="PriML Workshop" />
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Privacy in Machine Learning (NeurIPS 2019 Workshop)</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <link href="css/style.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

    <!-- Navigation -->
    <nav class="navbar navbar-custom navbar-fixed-top">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
                    Menu <i class="fa fa-bars"></i>
                </button>
                <a class="navbar-brand page-scroll" href="#page-top">
                    <span class="light">P<span style="text-transform:lowercase">ri</span>ML'19</span>
                </a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-right navbar-main-collapse">
                <ul class="nav navbar-nav">
                    <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                    <li class="hidden">
                        <a href="#page-top"></a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#about">Scope</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#dates">CFP &amp Dates</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#speakers">Invited Speakers</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#schedule">Schedule</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#papers">Accepted Papers</a>
                    </li>
                    <!-- <li>
                        <a class="page-scroll" href="#grants">Travel Grants</a>
                    </li> -->
                    <li>
                        <a class="page-scroll" href="#organizers">Organizers</a>
                    </li>
                    <li>
                      <a class="page-scroll" href="#access">Accessibility</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Intro Header -->
    <header class="intro">
        <div class="intro-body">
            <div class="container">
                <div class="row">
                    <div class="col-md-8 col-md-offset-2">
                        <h1 class="brand-heading">Privacy in Machine Learning</h1>
                        <p class="intro-text">
                            <a href="https://neurips.cc/" style="color:white">NeurIPS 2019</a> Workshop
                            <br />Vancouver, December 14
                        </p>
                        <!--<p class="location-text">
                            Palais des Congrès de Montréal
                            <br /> Room: 512CDGH
                        </p>-->
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- About Section -->
    <section id="about" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Scope</h2>

                  <p>The goal of our workshop is to bring together privacy experts working in academia and
                  industry to discuss the present and the future of privacy-aware technologies powered by
                  machine learning. The workshop will focus on the technical aspects of privacy research and
                  deployment with invited and contributed talks by distinguished researchers in the area.
                  We encourage submissions exploring a broad range of research areas related to data privacy,
                  including but not limited to:</p>

                  <ul class="list-group">
                      <li class="list-group-item speaker">Differential privacy: theory, applications, and implementations</li>
                      <li class="list-group-item speaker">Privacy-preserving machine learning</li>
                      <li class="list-group-item speaker">Trade-offs between privacy and utility</li>
                      <li class="list-group-item speaker">Programming languages for privacy-preserving data analysis</li>
                      <li class="list-group-item speaker">Statistical and information-theoretic notions of privacy</li>
                      <li class="list-group-item speaker">Empirical and theoretical comparisons between different notions of privacy</li>
                      <li class="list-group-item speaker">Privacy attacks</li>
                      <li class="list-group-item speaker">Policy-making aspects of data privacy</li>
                      <li class="list-group-item speaker">Secure multi-party computation techniques for machine learning</li>
                      <li class="list-group-item speaker">Learning on encrypted data, homomorphic encryption</li>
                      <li class="list-group-item speaker">Distributed privacy-preserving algorithms</li>
                      <li class="list-group-item speaker">Privacy in autonomous systems</li>
                      <li class="list-group-item speaker">Online social networks privacy</li>
                      <li class="list-group-item speaker">Interplay between privacy and adversarial robustness in machine learning</li>
                      <li class="list-group-item speaker">Relations between privacy, fairness and transparency</li>
                  </ul>
            </div>
        </div>
    </section>

    <!-- CFP & Dates Section -->
        <section id="dates" class="container content-section text-center">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2">
                    <h2>Call For Papers &amp; Important Dates</h2>
                    <a href="cfp-priml19.txt" class="btn btn-default btn-lg">Download Full CFP</a>
                    <br/>
                    <br/>
                    <br/>
                    <p>
                        <b>Submission deadline</b>: September 9, 2019, 23:59 UTC
                        <br/><b>Notification of acceptance</b>: October 1, 2019
                        <br/><b>NeurIPS early <a href="https://nips.cc/Conferences/2019/Press?article=2299">registration</a> deadline</b>: October 23, 2019
                        <br/><b>Workshop</b>: December 14, 2019 (Saturday)
                    </p>
                    <h3>Instructions</h3>
                      <p>
                        The submission deadline has now passed.
                        If your submission was accepted for a poster presentation,
                        please make your posters 36W x 48H inches or 90 x 122 cm.
                        Posters should be on light weight paper and should not be laminated.
                        As you design your poster,
                        you may find the following resource helpful:
                        <a href="resources/accessibility_posters_gilson2019.pdf">Guidelines for Creating Accessible Printed Posters</a>.

                      </p>
                    <!-- <a href="https://easychair.org/conferences/?conf=priml2019" class="btn btn-default btn-lg">Submit Your Abstract</a> -->
                    <br/>
                    <br/>
                    <br/>
                    <h3>Related Workshops</h3>
                        <p><b><a href="http://federated-learning.org/fl-neurips-2019/">Federated Learning for Data Privacy and Confidentiality</a> @ NeurIPS</b>: December 13, 2019</p>
                </div>
            </div>
        </section>

    <!-- Speakers Section -->
    <section id="speakers" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Invited Speakers</h2>
                <ul class="list-group">
                    <li class="list-group-item speaker">Philip Leclerc (US Census)</li>
                    <li class="list-group-item speaker">Ashwin Machanavajjhala (Duke University)</li>
                    <li class="list-group-item speaker">Brendan McMahan (Google)</li>
                    <li class="list-group-item speaker">Lalitha Sankar (Arizona State University)</li>
                </ul>
            </div>
        </div>
    </section>

    <!-- Schedule Section -->
    <section id="schedule" class="container content-section text-center">
        <div class="row">
            <div class="col-sm-8 col-sm-offset-2">
                <h2>Schedule</h2>
                <table class="table schedule">
                    <tbody>
                    <tr>
                        <td class="time">8:10</td>
                        <td class="slot">Opening</td>
                    </tr>

                    <tr>
                        <td class="time">8:15</td>
                        <td class="slot talk">
                        Invited talk:
                        <a href="#tabs2" data-toggle="collapse" class="accordion-toggle">
                        Brendan McMahan
                        <!-- &mdash;
                        title -->
                        </a>&nbsp;&nbsp;
                        <!-- <a href="slides/slides.pdf" class="link-paper">[slides]</a> -->
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs2">
                            More details coming soon
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">9:05</td>
                        <td class="slot talk"><a href="#tabs3" data-toggle="collapse" class="accordion-toggle">
                        Gaussian Differential Privacy
                        </a>
                        (contributed talk)
                        &nbsp;&nbsp;
                        <br/>
                        <span style="font-weight: normal">
                        Jinshuo Dong, Aaron Roth and Weijie Su
                        </span>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs3">
                              Differential privacy has seen remarkable success as a rigorous and practical formalization of data privacy in the past decade.
                              This privacy definition and its divergence based relaxations, however, have several acknowledged weaknesses, either in handling composition of private algorithms or in analyzing important primitives like privacy amplification by subsampling.
                              Inspired by the hypothesis testing formulation of privacy, this paper proposes a new relaxation, which we term &#10077;f-differential privacy&#10078; (f-DP).
                              This notion of privacy has a number of appealing properties and, in particular, avoids difficulties associated with divergence based relaxations.
                              First, f-DP preserves the hypothesis testing interpretation.
                              In addition, f-DP allows for lossless reasoning about composition in an algebraic fashion.
                              Moreover, we provide a powerful technique to import existing results proven for original DP to f-DP and, as an application, obtain a simple subsampling theorem for f-DP.
                              In addition to the above findings, we introduce a canonical single-parameter family of privacy notions within the f-DP class that is referred to as &#10077;Gaussian differential privacy&#10078; (GDP),
                              defined based on testing two shifted Gaussians.
                              GDP is focal among the f-DP class because of a central limit theorem we prove.
                              More precisely, the privacy guarantees of any hypothesis testing based definition of privacy (including original DP) converges to GDP in the limit under composition.
                              The CLT also yields a computationally inexpensive tool for analyzing the exact composition of private algorithms.
                              Taken together, this collection of attractive properties render f-DP a mathematically coherent, analytically tractable, and versatile framework for private data analysis.
                              Finally, we demonstrate the use of the tools we develop by giving an improved privacy analysis of noisy stochastic gradient descent.
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">9:25</td>
                        <td class="slot talk"><a href="#tabs4" data-toggle="collapse" class="accordion-toggle">
                        QUOTIENT: Two-Party Secure Neural Network Training &amp; Prediction
                        </a>
                        (contributed talk)
                        &nbsp;&nbsp;
                        <br/>
                        <span style="font-weight: normal">
                        Nitin Agrawal, Ali Shahin Shamsabadi, Matthew Kusner and Adria Gascon
                        </span>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs4">
                                Recently, there has been a wealth of effort devoted to the design of secure protocols for machine learning tasks.
                                Much of this is aimed at enabling secure prediction from highly-accurate Deep Neural Networks (DNNs).
                                However, as DNNs are trained on data, a key question is how such models can be also trained securely.
                                The few prior works on secure DNN training have focused either on designing custom protocols for existing training algorithms
                                or on developing tailored training algorithms and then applying generic secure protocols.
                                In this work, we investigate the advantages of designing training algorithms alongside a novel secure protocol, incorporating optimizations on both fronts.
                                We present QUOTIENT, a new method for discretized training of DNNs,
                                along with a customized secure two-party protocol for it.
                                QUOTIENT incorporates key components of state-of-the-art DNN training such as layer normalization and adaptive gradient methods,
                                and improves upon the state-of-the-art in DNN training in two-party computation.
                                Compared to prior work, we obtain an improvement of 50X in WAN time and 6&#37; in absolute accuracy.
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">9:45</td>
                        <td class="break">Coffee break</td>
                    </tr>

                    <tr>
                        <td class="time">10:30</td>
                        <td class="slot talk">
                        Invited talk:
                        <a href="#tabs5" data-toggle="collapse" class="accordion-toggle">
                        Ashwin Machanavajjhala
                        <!-- &mdash;
                        title -->
                        </a> &nbsp;&nbsp;
                        <!-- <a href="slides/chaudhuri.pdf" class="link-paper">[slides]</a> -->
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs5">
                              More details coming soon
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">11:20</td>
                        <td class="slot talk"><a href="#tabs6" data-toggle="collapse" class="accordion-toggle">
                        Spotlight talks
                        </a> &nbsp;&nbsp;
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs6">
                            <ol>
                                <li>[Jonathan Lebensold, William Hamilton, Borja Balle and Doina Precup] Actor Critic with Differentially Private Critic (#08)</li>
                                <li>[Andres Munoz, Umar Syed, Sergei Vassilvitskii and Ellen Vitercik] Private linear programming without constraint violations (#17)</li>
                                <li>[Ios Kotsogiannis, Yuchao Tao, Xi He, Ashwin Machanavajjhala, Michael Hay and Gerome Miklau] PrivateSQL: A Differentially Private SQL Query Engine (#27)</li>
                                <li>[Amrita Roy Chowdhury, Chenghong Wang, Xi He, Ashwin Machanavajjhala and Somesh Jha] Crypt$\epsilon$: Crypto-Assisted Differential Privacy on Untrusted Servers (#31)</li>
                                <li>[Jiaming Xu and Dana Yang] Optimal Query Complexity of Private Sequential Learning (#32)</li>
                                <li>[Hsiang Hsu, Shahab Asoodeh and Flavio Calmon] Discovering Information-Leaking Samples and Features (#43)</li>
                                <li>[Martine De Cock, Rafael Dowsley, Anderson Nascimento, Davis Railsback, Jianwei Shen and Ariel Todoki] Fast Secure Logistic Regression for High Dimensional Gene Data (#44)</li>
                                <li>[Giuseppe Vietri, Grace Tian, Mark Bun, Thomas Steinke and Steven Wu] New Oracle-Efficient Algorithms for Private Synthetic Data Release (#45)</li>
                            </ol>
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">11:30</td>
                        <td class="slot">Poster session</td>
                    </tr>

                    <tr>
                        <td class="time">12:30</td>
                        <td class="break">Lunch break</td>
                    </tr>

                    <tr>
                        <td class="time">14:00</td>
                        <td class="slot talk">
                        Invited talk:
                        <a href="#tabs7" data-toggle="collapse" class="accordion-toggle">
                        Lalitha Sankar
                        <!-- &mdash;
                        title -->
                        </a> &nbsp;&nbsp;
                        <!-- <a href="slides/smith.pdf" class="link-paper">[slides]</a> -->
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs7">
                            More details coming soon
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">14:50</td>
                        <td class="slot talk"><a href="#tabs8" data-toggle="collapse" class="accordion-toggle">
                        Pan-Private Uniformity Testing
                        </a>
                        (contributed talk)
                        &nbsp;&nbsp;
                        <br/>
                        <span style="font-weight: normal">
                        Kareem Amin, Matthew Joseph and Jieming Mao
                        </span>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs8">
                              A centrally differentially private algorithm maps raw data to differentially private outputs.
                              In contrast, a locally differentially private algorithm
                              may only access data through public interaction with data holders,
                              and this interaction must be a differentially private function of the data.
                              We study the intermediate model of pan-privacy.
                              Unlike a locally private algorithm, a pan-private algorithm receives data in the clear.
                              Unlike a centrally private algorithm,
                              the algorithm receives data one element at a time
                              and must maintain a differentially private internal state while processing this stream.
                              First, we show that pan-privacy against multiple intrusions on the internal state is
                              equivalent to sequentially interactive local privacy.
                              Next, we contextualize pan-privacy against a single intrusion
                              by analyzing the sample complexity of uniformity testing over domain [k].
                              Focusing on the dependence on k,
                              centrally private uniformity testing has sample complexity &Theta;(&radic;k),
                              while noninteractive locally private uniformity testing has sample complexity &Theta;(k).
                              We show that the sample complexity of pan-private uniformity testing is &Theta;(k<sup>2/3</sup>).
                              By a new &Omega;(k) lower bound for the sequentially interactive setting,
                              we also separate pan-private from sequentially interactive locally private
                              and multi-intrusion pan-private uniformity testing.
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">15:10</td>
                        <td class="slot talk"><a href="#tabs9" data-toggle="collapse" class="accordion-toggle">
                        Private Stochastic Convex Optimization: Optimal Rates in Linear Time
                        </a>
                        (contributed talk)
                        &nbsp;&nbsp;
                        <!-- <a href="slides/39.pdf" class="link-paper">[slides]</a> -->
                        <br/>
                        <span style="font-weight: normal">
                        Vitaly Feldman, Tomer Koren and Kunal Talwar
                        </span>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs9">
                            More details coming soon
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">15:30</td>
                        <td class="break">Coffee break</td>
                    </tr>

                    <tr>
                        <td class="time">16:15</td>
                        <td class="slot talk">
                        Invited talk:
                        <a href="#tabs10" data-toggle="collapse" class="accordion-toggle">
                        Philip Leclerc
                        <!-- &mdash;
                        title -->
                        </a> &nbsp;&nbsp;
                        <!-- <a href="slides/chaudhuri.pdf" class="link-paper">[slides]</a> -->
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs10">
                            More details coming soon
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">17:05</td>
                        <td class="slot">Panel Discussion</td>
                    </tr>

                    <tr>
                        <td class="time">17:55</td>
                        <td class="slot">Closing</td>
                    </tr>

        		    </tbody>
        		</table>
            </div>
        </div>
    </section>

    <!-- Accepted Papers -->

    <section id="papers" class="container content-section text-center">
        <div class="row">
        <div class="col-lg-8 col-lg-offset-2">
            <h2>Accepted Papers</h2>
                <h4 style="color: #d07200;">
                Links to pdfs as well as abstracts will be added soon.
                </h4>

                <!-- <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            AUTHOTS
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs8" class="paper-title">
                            TITLE (paper with PDF)
                        </a> &nbsp;&nbsp;
                        <a href="papers/13.pdf" class="link-paper">[PDF]</a>
                    </div>
                    <div id="abs8" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        AUTHORS
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs1" class="paper-title">
                        TITLE (paper with Arvix link)
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1811.11124" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs1" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            AUTHORS
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs11" class="paper-title">
                            TITLE (paper with contributed talk) <font color="#d07200"><b>(contributed talk)</b></font>
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1806.03287" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs11" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div>
                </div> -->

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Clément Canonne, Gautam Kamath, Audra McMillan, Jonathan Ullman and Lydia Zakynthinou
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs1" class="paper-title">
                        Private Identity Testing for High-Dimensional Distributions
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/pdf/1905.11947.pdf" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs1" class="panel-footer panel-paper-footer collapse">
                    In this work we present novel differentially private identity (goodness-of-fit) testers for natural and widely studied classes of multivariate product distributions:
                    Gaussians in &#8477;<sup>d</sup> with known covariance and product distributions over {&plusmn;1}<sup>d</sup>.
                    Our testers have improved sample complexity compared to those derived from previous techniques,
                    and are the first testers whose sample complexity matches the order-optimal minimax sample complexity of O(d<sup>1/2</sup>&#47;&alpha;<sup>2</sup>) in many parameter regimes.
                    We construct two types of testers, exhibiting tradeoffs between sample complexity and computational complexity.
                    Finally, we provide a two-way reduction between testing a subclass of multivariate product distributions and testing univariate distributions,
                    and thereby obtain upper and lower bounds for testing this subclass of product distributions.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                          Kwang-Sung Jun and Francesco Orabona
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs2" class="paper-title">
                        Parameter-Free Locally Differentially Private Stochastic Subgradient Descent
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1911.09564" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs2" class="panel-footer panel-paper-footer collapse">
                    We consider the problem of minimizing a convex risk with stochastic subgradients guaranteeing &#x3F5;-locally differentially private (&#x3F5;-LDP).
                    While it has been shown that stochastic optimization is possible with &#x3F5;-LDP via the standard SGD (Song et al., 2013),
                    its convergence rate largely depends on the learning rate, which must be tuned via repeated runs.
                    Further, tuning is detrimental to privacy loss since it significantly increases the number of gradient requests.
                    In this work, we propose BANCO (Betting Algorithm for Noisy COins),
                    the first &#x3F5;-LDP SGD algorithm that essentially matches the convergence rate of the tuned SGD without any learning rate parameter,
                    reducing privacy loss and saving privacy budget.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Seth Neel, Zhiwei Steven Wu, Aaron Roth and Giuseppe Vietri
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs3" class="paper-title">
                        Differentially Private Objective Perturbation: Beyond Smoothness and Convexity
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1909.01783" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs3" class="panel-footer panel-paper-footer collapse">
                    One of the most effective algorithms for differentially private learning and optimization is objective perturbation.
                    This technique augments a given optimization problem (e.g. deriving from an ERM problem) with a random linear term,
                    and then exactly solves it.
                    However, to date, analyses of this approach crucially rely on the convexity and smoothness of the objective function.
                    We give two algorithms that extend this approach substantially.
                    The first algorithm requires nothing except boundedness of the loss function,
                    and operates over a discrete domain.
                    Its privacy and accuracy guarantees hold even without assuming convexity.
                    The second algorithm operates over a continuous domain and requires only that the loss function be bounded and Lipschitz in its continuous parameter.
                    Its privacy analysis does not even require convexity.
                    Its accuracy analysis does require convexity,
                    but does not require second order conditions like smoothness.
                    We complement our theoretical results with an empirical evaluation of the non-convex case,
                    in which we use an integer program solver as our optimization oracle.
                    We find that for the problem of learning linear classifiers,
                    directly optimizing for 0/1 loss using our approach can out-perform the more standard approach of privately optimizing a convex-surrogate loss function on the Adult dataset.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Di Wang, Huanyu Zhang, Marco Gaboardi and Jinhui Xu
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs4" class="paper-title">
                        Estimating Smooth GLM in Non-interactive Local Differential Privacy Model with Public Unlabeled Data
                        </a> &nbsp;&nbsp;
                        <!-- <a href="https://arxiv.org" class="link-paper">[arxiv]</a> -->
                    </div>
                    <!-- <div id="abs4" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div> -->
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Liwei Song, Reza Shokri and Prateek Mittal
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs5" class="paper-title">
                        Privacy vs Robustness (against Adversarial Examples) in Machine Learning
                        </a> &nbsp;&nbsp;
                        <!-- <a href="https://arxiv.org" class="link-paper">[arxiv]</a> -->
                    </div>
                    <!-- <div id="abs5" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div> -->
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Jonathan Lebensold, William Hamilton, Borja Balle and Doina Precup
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs6" class="paper-title">
                        Actor Critic with Differentially Private Critic
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1910.05876" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs6" class="panel-footer panel-paper-footer collapse">
                    Reinforcement learning algorithms are known to be sample inefficient, and often performance on one task can be substantially improved by leveraging information (e.g., via pre-training) on other related tasks.
                    In this work, we propose a technique to achieve such knowledge transfer in cases where agent trajectories contain sensitive or private information, such as in the healthcare domain.
                    Our approach leverages a differentially private policy evaluation algorithm to initialize an actor-critic model and improve the effectiveness of learning in downstream tasks.
                    We empirically show this technique increases sample efficiency in resource-constrained control problems while preserving the privacy of trajectories collected in an upstream task.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Samyadeep Basu, Rauf Izmailov and Chris Mesterharm
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs7" class="paper-title">
                        Membership Model Inversion Attacks for Deep Networks
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/pdf/1910.04257.pdf" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs7" class="panel-footer panel-paper-footer collapse">
                    With the increasing adoption of AI,
                    inherent security and privacy vulnerabilities for machine learning systems are being discovered.
                    One such vulnerability makes it possible for an adversary to obtain private information about the types of instances used to train the targeted machine learning model.
                    This so-called model inversion attack is based on sequential leveraging of classification scores towards obtaining high confidence representations for various classes.
                    However, for deep networks, such procedures usually lead to unrecognizable representations that are useless for the adversary.
                    In this paper, we introduce a more realistic definition of model inversion,
                    where the adversary is aware of the general purpose of the attacked model
                    (for instance, whether it is an OCR system or a facial recognition system),
                    and the goal is to find realistic class representations within the corresponding lower-dimensional manifold
                    (of, respectively, general symbols or general faces).
                    To that end, we leverage properties of generative adversarial networks for constructing a connected lower-dimensional manifold,
                    and demonstrate the efficiency of our model inversion attack that is carried out within that manifold.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Gautam Kamath, Janardhan Kulkarni, Zhiwei Steven Wu and Huanyu Zhang
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs8" class="paper-title">
                        Privately Learning Markov Random Fields
                        </a> &nbsp;&nbsp;
                        <!-- <a href="https://arxiv.org" class="link-paper">[arxiv]</a> -->
                    </div>
                    <!-- <div id="abs8" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div> -->
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Lovedeep Gondara and Ke Wang
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs9" class="paper-title">
                        Differentially Private Survival Function Estimation
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/pdf/1910.05108.pdf" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs9" class="panel-footer panel-paper-footer collapse">
                    Survival function estimation is used in many disciplines,
                    but it is most common in medical analytics in the form of the Kaplan-Meier estimator.
                    Sensitive data (patient records) is used in the estimation without any explicit control on the information leakage,
                    which is a significant privacy concern.
                    We propose a first differentially private estimator of the survival function and
                    show that it can be easily extended to provide differentially private confidence intervals and test statistics
                    without spending any extra privacy budget.
                    We further provide extensions for differentially private estimation of the competing risk cumulative incidence function.
                    Using nine real-life clinical datasets,
                    we provide empirical evidence that our proposed method provides good utility while simultaneously providing strong privacy guarantees.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Ang Li, Jiayi Guo, Huanrui Yang and Yiran Chen
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs10" class="paper-title">
                        DeepObfuscator: Adversarial Training Framework for Privacy-Preserving Image Classification
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/pdf/1909.04126.pdf" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs10" class="panel-footer panel-paper-footer collapse">
                    Deep learning has been widely utilized in many computer vision applications and achieved remarkable commercial success.
                    However, running deep learning models on mobile devices is generally challenging due to limitation of the available computing resources.
                    It is common to let the users send their service requests to cloud servers that run the large-scale deep learning models to process.
                    Sending the data associated with the service requests to the cloud, however, impose risks on the user data privacy.
                    Some prior arts proposed sending the features extracted from raw data (e.g., images) to the cloud.
                    Unfortunately, these extracted features can still be exploited by attackers to recover raw images and to infer embedded private attributes (e.g., age, gender, etc.).
                    In this paper, we propose an adversarial training framework <i>DeepObfuscator</i> that can prevent extracted features from being utilized to reconstruct raw images and infer private attributes,
                    while retaining the useful information for the intended cloud service (i.e., image classification).
                    DeepObfuscator includes a learnable encoder,  namely,  obfuscator  that  is  designed  to  hide  privacy-related sensitive information from the features by performing our proposed adversarial training algorithm.
                    The proposed algorithm is designed by simulating the game between an attacker who makes efforts to reconstruct raw images and infer private attributes from the extracted features
                    and a defender who aims to protect user privacy.
                    Our experiments on CelebA dataset show that the quality of the reconstructed images from the obfuscated features of the raw image is dramatically decreased
                    from 0.9458 to 0.3175 in terms of multi-scale structural similarity (MS-SSIM).
                    The person in the reconstructed image, hence, becomes hardly to be re-identified.
                    The classification accuracy of the inferred private attributes that can be achieved by the attacker drops down to a random-guessing level, e.g.,
                    the accuracy of gender is reduced from 97.36&#37; to 58.85&#37;.
                    As a comparison, the accuracy of the intended classification tasks performed via the cloud service drops by only 2&#37;.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Fatemehsadat Mireshghallah, Mohammadkazem Taram, Prakash Ramrakhyani, Dean Tullsen and Hadi Esmaeilzadeh
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs11" class="paper-title">
                        Shredder: Learning Noise Distributions to Protect Inference Privacy
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1905.11814" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs11" class="panel-footer panel-paper-footer collapse">
                    Sheer amount of computation in deep neural networks has pushed their execution to the cloud.
                    This de facto cloud-hosted inference, however, raises serious privacy concerns as private data is communicated and stored in remote servers.
                    The data could be mishandled by cloud providers, used for unsolicited analytics, or simply compromised through network and system security vulnerability.
                    To that end, this paper devises SHREDDER that reduces the information content of the communicated data without diminishing the cloud's ability to maintain acceptably high accuracy.
                    To that end, SHREDDER learns two sets of noise distributions whose samples, named multiplicative and additive noise tensors, are applied to the communicated data while maintaining the inference accuracy.
                    The key idea is that SHREDDER learns these noise distributions offline without altering the topology or the weights of the pre-trained network.
                    SHREDDER repeatedly learns sample noise tensors from the distributions by casting the tensors as a set of trainable parameters while keeping the weights constant.
                    Since the key idea is learning the noise, we are able to devise a loss function that strikes a balance between accuracy and information degradation.
                    To this end, we use self-supervision to train the noise tensors to achieve an intermediate representation of the data that contains less private information.
                    Experimentation with real-world deep neural networks shows that, compared to the original execution, SHREDDER reduces the mutual information between the input and the communicated data by 66.90&#37;,
                    and yields a misclassification rate of 94.5&#37; over private labels, significantly reducing adversary's ability to infer private data,
                    while sacrificing only 1.74&#37; loss in accuracy without any knowledge about the private labels.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Jinshuo Dong, Aaron Roth and Weijie Su
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs12" class="paper-title">
                        Gaussian Differential Privacy <font color="#d07200"><b>(contributed talk)</b></font>
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1905.02383" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs12" class="panel-footer panel-paper-footer collapse">
                      Differential privacy has seen remarkable success as a rigorous and practical formalization of data privacy in the past decade.
                      This privacy definition and its divergence based relaxations, however, have several acknowledged weaknesses, either in handling composition of private algorithms or in analyzing important primitives like privacy amplification by subsampling.
                      Inspired by the hypothesis testing formulation of privacy, this paper proposes a new relaxation, which we term &#10077;f-differential privacy&#10078; (f-DP).
                      This notion of privacy has a number of appealing properties and, in particular, avoids difficulties associated with divergence based relaxations.
                      First, f-DP preserves the hypothesis testing interpretation.
                      In addition, f-DP allows for lossless reasoning about composition in an algebraic fashion.
                      Moreover, we provide a powerful technique to import existing results proven for original DP to f-DP and, as an application, obtain a simple subsampling theorem for f-DP.
                      In addition to the above findings, we introduce a canonical single-parameter family of privacy notions within the f-DP class that is referred to as &#10077;Gaussian differential privacy&#10078; (GDP),
                      defined based on testing two shifted Gaussians.
                      GDP is focal among the f-DP class because of a central limit theorem we prove.
                      More precisely, the privacy guarantees of any hypothesis testing based definition of privacy (including original DP) converges to GDP in the limit under composition.
                      The CLT also yields a computationally inexpensive tool for analyzing the exact composition of private algorithms.
                      Taken together, this collection of attractive properties render f-DP a mathematically coherent, analytically tractable, and versatile framework for private data analysis.
                      Finally, we demonstrate the use of the tools we develop by giving an improved privacy analysis of noisy stochastic gradient descent.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Andres Munoz, Umar Syed, Sergei Vassilvitskii and Ellen Vitercik
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs13" class="paper-title">
                        Private Linear Programming Without Constraint Violations
                        </a> &nbsp;&nbsp;
                        <!-- <a href="https://arxiv.org" class="link-paper">[arxiv]</a> -->
                    </div>
                    <!-- <div id="abs13" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div> -->
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Hafiz Imtiaz, Jafar Mohammadi and Anand D. Sarwate
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs14" class="paper-title">
                        Correlation-Assisted Distributed Differentially Private Estimation
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1904.10059" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs14" class="panel-footer panel-paper-footer collapse">
                    Many applications of machine learning, such as human health research, involve processing private or sensitive information.
                    Privacy concerns may impose significant hurdles to collaboration in scenarios where there are multiple sites holding data and the goal is to estimate properties jointly across all datasets.
                    Differentially private decentralized algorithms can provide strong privacy guarantees.
                    However, the accuracy of the joint estimates may be poor when the datasets at each site are small.
                    This paper proposes a new framework, Correlation Assisted Private Estimation (CAPE), for designing privacy-preserving decentralized algorithms with better accuracy guarantees in an honest-but-curious model.
                    CAPE can be used in conjunction with the functional mechanism for statistical and machine learning optimization problems.
                    A tighter characterization of the functional mechanism is provided that allows CAPE to achieve the same performance as a centralized algorithm in the decentralized setting using all datasets.
                    Empirical results on regression and neural network problems for both synthetic and real datasets show that differentially private methods can be competitive with non-private algorithms in many scenarios of interest.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Naoise Holohan, Stefano Braghin, Pol Mac Aonghusa and Killian Levacher
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs15" class="paper-title">
                        Diffprivlib: The IBM Differential Privacy Library
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1907.02444" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs15" class="panel-footer panel-paper-footer collapse">
                      Since its conception in 2006, differential privacy has emerged as the de-facto standard in data privacy, owing to its robust mathematical guarantees, generalised applicability and rich body of literature.
                      Over the years, researchers have studied differential privacy and its applicability to an ever-widening field of topics.
                      Mechanisms have been created to optimise the process of achieving differential privacy, for various data types and scenarios.
                      Until this work however, all previous work on differential privacy has been conducted on a ad-hoc basis, without a single, unifying codebase to implement results.
                      In this work, we present the IBM Differential Privacy Library, a general purpose, open source library for investigating, experimenting and developing differential privacy applications in the Python programming language.
                      The library includes a host of mechanisms, the building blocks of differential privacy, alongside a number of applications to machine learning and other data analytics tasks.
                      Simplicity and accessibility has been prioritised in developing the library, making it suitable to a wide audience of users, from those using the library for their first investigations in data privacy, to the privacy experts looking to contribute their own models and mechanisms for others to use.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Antti Koskela, Joonas Jälkö and Antti Honkela
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs16" class="paper-title">
                        Computing Exact Guarantees for Differential Privacy
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1906.03049" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs16" class="panel-footer panel-paper-footer collapse">
                    Differentially private (DP) machine learning has recently become popular.
                    The privacy loss of DP algorithms is commonly reported using (&epsilon;,&delta;)-DP.
                    In this paper, we propose a numerical accountant for evaluating the privacy loss for algorithms with continuous one dimensional output.
                    This accountant can be applied to the subsampled multidimensional Gaussian mechanism which underlies the popular DP stochastic gradient descent.
                    The proposed method is based on a numerical approximation of an integral formula which gives the exact (&epsilon;,&delta;)-values.
                    The approximation is carried out by discretising the integral and by evaluating discrete convolutions using the fast Fourier transform algorithm.
                    We give both theoretical error bounds and numerical error estimates for the approximation.
                    Experimental comparisons with state-of-the-art techniques demonstrate significant improvements in bound tightness and/or computation time.
                    Python code for the method can be found in Github.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Joonas Jälkö, Antti Honkela and Samuel Kaski
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs17" class="paper-title">
                        Privacy-Preserving Data Sharing via Probabilistic Modelling
                        </a> &nbsp;&nbsp;
                        <!-- <a href="https://arxiv.org" class="link-paper">[arxiv]</a> -->
                    </div>
                    <!-- <div id="abs17" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div> -->
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Nitin Agrawal, Ali Shahin Shamsabadi, Matthew Kusner and Adria Gascon
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs18" class="paper-title">
                        QUOTIENT: Two-Party Secure Neural Network Training and Prediction
                        <font color="#d07200"><b>(contributed talk)</b></font>
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/pdf/1907.03372.pdf" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs18" class="panel-footer panel-paper-footer collapse">
                      Recently, there has been a wealth of effort devoted to the design of secure protocols for machine learning tasks.
                      Much of this is aimed at enabling secure prediction from highly-accurate Deep Neural Networks (DNNs).
                      However, as DNNs are trained on data, a key question is how such models can be also trained securely.
                      The few prior works on secure DNN training have focused either on designing custom protocols for existing training algorithms
                      or on developing tailored training algorithms and then applying generic secure protocols.
                      In this work, we investigate the advantages of designing training algorithms alongside a novel secure protocol, incorporating optimizations on both fronts.
                      We present QUOTIENT, a new method for discretized training of DNNs,
                      along with a customized secure two-party protocol for it.
                      QUOTIENT incorporates key components of state-of-the-art DNN training such as layer normalization and adaptive gradient methods,
                      and improves upon the state-of-the-art in DNN training in two-party computation.
                      Compared to prior work, we obtain an improvement of 50X in WAN time and 6&#37; in absolute accuracy.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Dingfan Chen, Ning Yu, Yang Zhang and Mario Fritz
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs19" class="paper-title">
                        GAN-Leaks: A Taxonomy of Membership Inference Attacks against GANs
                        </a> &nbsp;&nbsp;
                        <!-- <a href="https://arxiv.org" class="link-paper">[arxiv]</a> -->
                    </div>
                    <!-- <div id="abs19" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div> -->
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Si Kai Lee, Luigi Gresele, Mijung Park and Krikamol Muandet
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs20" class="paper-title">
                        Private Causal Inference using Propensity Scores
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1905.12592" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs20" class="panel-footer panel-paper-footer collapse">
                    The use of inverse probability weighting (IPW) methods to
                    estimate the causal effect of treatments from observational
                    studies is widespread in econometrics, medicine and social
                    sciences. Although these studies often involve sensitive
                    information, thus far there has been no work on
                    privacy-preserving IPW methods. We address this by
                    providing a novel framework for privacy-preserving IPW
                    (PP-IPW) methods. We include a theoretical analysis of the
                    effects of our proposed privatisation procedure on the
                    estimated average treatment effect, and evaluate our PP-IPW
                    framework on synthetic, semi-synthetic and real datasets.
                    The empirical results are consistent with our theoretical
                    findings.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Kareem Amin, Matthew Joseph and Jieming Mao
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs21" class="paper-title">
                        Pan-Private Uniformity Testing <font color="#d07200"><b>(contributed talk)</b></font>
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1911.01452" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs21" class="panel-footer panel-paper-footer collapse">
                      A centrally differentially private algorithm maps raw data to differentially private outputs.
                      In contrast, a locally differentially private algorithm
                      may only access data through public interaction with data holders,
                      and this interaction must be a differentially private function of the data.
                      We study the intermediate model of pan-privacy.
                      Unlike a locally private algorithm, a pan-private algorithm receives data in the clear.
                      Unlike a centrally private algorithm,
                      the algorithm receives data one element at a time
                      and must maintain a differentially private internal state while processing this stream.
                      First, we show that pan-privacy against multiple intrusions on the internal state is
                      equivalent to sequentially interactive local privacy.
                      Next, we contextualize pan-privacy against a single intrusion
                      by analyzing the sample complexity of uniformity testing over domain [k].
                      Focusing on the dependence on k,
                      centrally private uniformity testing has sample complexity &Theta;(&radic;k),
                      while noninteractive locally private uniformity testing has sample complexity &Theta;(k).
                      We show that the sample complexity of pan-private uniformity testing is &Theta;(k<sup>2/3</sup>).
                      By a new &Omega;(k) lower bound for the sequentially interactive setting,
                      we also separate pan-private from sequentially interactive locally private
                      and multi-intrusion pan-private uniformity testing.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Ios Kotsogiannis, Yuchao Tao, Xi He, Ashwin Machanavajjhala, Michael Hay and Gerome Miklau
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs22" class="paper-title">
                        PrivateSQL: A Differentially Private SQL Query Engine
                        </a> &nbsp;&nbsp;
                        <a href="http://www.vldb.org/pvldb/vol12/p1371-kotsogiannis.pdf" class="link-paper">[pdf]</a>
                    </div>
                    <div id="abs22" class="panel-footer panel-paper-footer collapse">
                    Differential privacy is considered a de facto standard for private data analysis.
                    However, the definition and much of the supporting literature applies to flat tables.
                    While there exist variants of the definition and specialized algorithms for specific types of relational data (e.g. graphs),
                    there isn’t a general privacy definition for multi-relational schemas with constraints,
                    and no system that permits accurate differentially private answering of SQL queries
                    while imposing a fixed privacy budget across all queries posed by the analyst.
                    This work presents PrivateSQL, a first-of-its-kind end-to-end differentially private relational database system.
                    PrivateSQL allows an analyst to query data stored in a standard database management system
                    using a rich class of SQL counting queries.
                    PrivateSQL adopts a novel generalization of differential privacy to multi-relational data
                    that takes into account constraints in the schema like foreign keys,
                    and allows the data owner to flexibly specify entities in the schema that need privacy.
                    PrivateSQL ensures a fixed privacy loss across all the queries posed by the analyst
                    by answering queries on private synopses generated from several views over the base relation
                    that are tuned to have low error on a representative query workload.
                    We experimentally evaluate PrivateSQL on a real-world dataset and a work-load of more than 3,600 queries.
                    We show that for 50&#37; of the queries PrivateSQL offers at least 1,000x better error rates
                    than solutions adapted from prior work.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Chao Jin, Ahmad Qaisar Ahmad Al Badawi, Balagopal Unnikrishnan, Jie Lin, Fook Mun Chan, James Brown, J. Peter Campbell, Michael F. Chiang, Jayashree Kalpathy-Cramer, Vijay Chandrasekhar, Pavitra Krishnaswamy and Khin Mi Mi Aung
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs23" class="paper-title">
                        CareNets: Efficient Homomorphic CNN for High Resolution Images
                        </a> &nbsp;&nbsp;
                        <!-- <a href="https://arxiv.org" class="link-paper">[arxiv]</a> -->
                    </div>
                    <!-- <div id="abs23" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div> -->
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Alexandra Schofield, Gregory Yauney and David Mimno
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs24" class="paper-title">
                        Combatting The Challenges of Local Privacy for Distributional Semantics with Compression
                        </a> &nbsp;&nbsp;
                        <!-- <a href="https://arxiv.org" class="link-paper">[arxiv]</a> -->
                    </div>
                    <!-- <div id="abs24" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div> -->
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Vitaly Feldman, Tomer Koren and Kunal Talwar
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs25" class="paper-title">
                        Private Stochastic Convex Optimization: Optimal Rates in Linear Time
                        <font color="#d07200"><b>(contributed talk)</b></font>
                        </a> &nbsp;&nbsp;
                        <!-- <a href="https://arxiv.org" class="link-paper">[arxiv]</a> -->
                    </div>
                    <!-- <div id="abs25" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div> -->
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Amrita Roy Chowdhury, Chenghong Wang, Xi He, Ashwin Machanavajjhala and Somesh Jha
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs26" class="paper-title">
                        Crypt&#949;: Crypto-Assisted Differential Privacy on Untrusted Servers
                        </a> &nbsp;&nbsp;
                        <!-- <a href="https://arxiv.org" class="link-paper">[arxiv]</a> -->
                    </div>
                    <!-- <div id="abs26" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div> -->
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Jiaming Xu and Dana Yang
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs27" class="paper-title">
                        Optimal Query Complexity of Private Sequential Learning
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1909.09836" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs27" class="panel-footer panel-paper-footer collapse">
                      Motivated by privacy concerns in many practical applications such as Federated Learning,
                      we study a stylized private sequential learning problem:
                      a learner tries to estimate an unknown scalar value,
                      by sequentially querying an external database and receiving binary responses;
                      meanwhile, a third-party adversary observes the learner's queries but not the responses.
                      The learner's goal is to design a querying strategy with the minimum number of queries
                      (optimal query complexity) so that she can accurately estimate the true value,
                      while the adversary even with the complete knowledge of her querying strategy cannot.
                      Prior work has obtained both upper and lower bounds on the optimal query complexity,
                      however, these upper and lower bounds have a large gap in general.
                      In this paper, we construct new querying strategies and prove almost matching upper and lower bounds,
                      providing a complete characterization of the optimal query complexity
                      as a function of the estimation accuracy and the desired levels of privacy.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Benjamin Spector, Andrew Tomkins and Ravi Kumar
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs28" class="paper-title">
                        Preventing Adversarial Use of Datasets through Fair Core-set Construction
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1910.10871" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs28" class="panel-footer panel-paper-footer collapse">
                    We propose improving the privacy properties of a dataset by publishing only a strategically chosen "core-set" of the data containing a subset of the instances.
                    The core-set allows strong performance on primary tasks, but forces poor performance on unwanted tasks.
                    We give methods for both linear models and neural networks and demonstrate their efficacy on data.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Nhathai Phan, My Thai, Devu Shila and Ruoming Jin
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs29" class="paper-title">
                        Differentially Private Lifelong Learning
                        </a> &nbsp;&nbsp;
                        <!-- <a href="https://arxiv.org" class="link-paper">[arxiv]</a> -->
                    </div>
                    <!-- <div id="abs29" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div> -->
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Alessandro Epasto, Hossein Esfandiari, Vahab Mirrokni, Andreas Munoz Medina, Umar Syed and Sergei Vassilvitskii
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs30" class="paper-title">
                        Anonymizing List Data
                        </a> &nbsp;&nbsp;
                        <!-- <a href="https://arxiv.org" class="link-paper">[arxiv]</a> -->
                    </div>
                    <!-- <div id="abs30" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div> -->
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Mimansa Jaiswal and Emily Mower Provost
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs31" class="paper-title">
                        Privacy Enhanced Multimodal Neural Representations for Emotion Recognition
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1910.13212" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs31" class="panel-footer panel-paper-footer collapse">
                    Many mobile applications and virtual conversational agents now aim to recognize and adapt to emotions.
                    To enable this, data are transmitted from users' devices and stored on central servers.
                    Yet, these data contain sensitive information that could be used by mobile applications without user's consent or, maliciously, by an eavesdropping adversary.
                    In this work, we show how multimodal representations trained for a primary task, here emotion recognition, can unintentionally leak demographic information, which could override a selected opt-out option by the user.
                    We analyze how this leakage differs in representations obtained from textual, acoustic, and multimodal data.
                    We use an adversarial learning paradigm to unlearn the private information present in a representation and investigate the effect of varying the strength of the adversarial component on the primary task and on the privacy metric, defined here as the inability of an attacker to predict specific demographic information.
                    We evaluate this paradigm on multiple datasets and show that we can improve the privacy metric while not significantly impacting the performance on the primary task.
                    To the best of our knowledge, this is the first work to analyze how the privacy metric differs across modalities and how multiple privacy concerns can be tackled while still maintaining performance on emotion recognition.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Mrinank Sharma, Michael Hutchinson, Siddharth Swaroop, Antti Honkela and Richard Turner
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs32" class="paper-title">
                        Differentially Private Federated Variational Inference
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1911.10563" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs32" class="panel-footer panel-paper-footer collapse">
                    In many real-world applications of machine learning, data are distributed across many clients and cannot leave the devices they are stored on.
                    Furthermore, each client's data, computational resources and communication constraints may be very different.
                    This setting is known as federated learning, in which privacy is a key concern.
                    Differential privacy is commonly used to provide mathematical privacy guarantees.
                    This work, to the best of our knowledge, is the first to consider federated, differentially private, Bayesian learning.
                    We build on Partitioned Variational Inference (PVI) which was recently developed to support approximate Bayesian inference in the federated setting.
                    We modify the client-side optimisation of PVI to provide an (&varepsilon;, &delta;)-DP guarantee.
                    We show that it is possible to learn moderately private logistic regression models in the federated setting that achieve similar performance to models trained non-privately on centralised data.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Hassan Takabi, Robert Podschwadt, Jeff Druce, Curt Wu and Kevin Procopio
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs33" class="paper-title">
                        Privacy preserving Neural Network Inference on Encrypted Data with GPUs
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1911.11377" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs33" class="panel-footer panel-paper-footer collapse">
                      Machine Learning as a Service (MLaaS) has become a growing trend in recent years and several such services are currently offered.
                      MLaaS is essentially a set of services that provides machine learning tools and capabilities as part of cloud computing services.
                      In these settings, the cloud has pre-trained models that are deployed and large computing capacity whereas the clients can use these models to make predictions without having to worry about maintaining the models and the service.
                      However, the main concern with MLaaS is the privacy of the client's data.
                      Although there have been several proposed approaches in the literature to run machine learning models on encrypted data, the performance is still far from being satisfactory for practical use.
                      In this paper, we aim to accelerate the performance of running machine learning on encrypted data using combination of Fully Homomorphic Encryption (FHE), Convolutional Neural Networks (CNNs) and Graphics Processing Units (GPUs).
                      We use a number of optimization techniques, and efficient GPU-based implementation to achieve high performance.
                      We evaluate a CNN whose architecture is similar to AlexNet to classify homomorphically encrypted samples from the Cars Overhead With Context (COWC) dataset.
                      To the best of our knowledge, it is the first time such a complex network and large dataset is evaluated on encrypted data.
                      Our approach achieved reasonable classification accuracy of 95% for the COWC dataset.
                      In terms of performance, our results show that we could achieve several thousands times speed up when we implement GPU-accelerated FHE operations on encrypted floating point numbers.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Casey Meehan and Kamalika Chaudhuri
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs34" class="paper-title">
                        Location Trace Privacy Under Conditional Priors
                        </a> &nbsp;&nbsp;
                        <!-- <a href="https://arxiv.org" class="link-paper">[arxiv]</a> -->
                    </div>
                    <!-- <div id="abs34" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div> -->
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Zhengli Zhao, Nicolas Papernot, Sameer Singh, Neoklis Polyzotis and Augustus Odena
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs35" class="paper-title">
                        Improving Differentially Private Models via Active Learning
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1910.01177" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs35" class="panel-footer panel-paper-footer collapse">
                    Broad adoption of machine learning techniques has increased privacy concerns for models trained on sensitive data such as medical records.
                    Existing techniques for training differentially private (DP) models give rigorous privacy guarantees,
                    but applying these techniques to neural networks can severely degrade model performance.
                    This performance reduction is an obstacle to deploying private models in the real world.
                    In this work, we improve the performance of DP models by fine-tuning them through active learning on public data.
                    We introduce two new techniques - DIVERSEPUBLIC and NEARPRIVATE - for doing this fine-tuning in a privacy-aware way.
                    For the MNIST and SVHN datasets, these techniques improve state-of-the-art accuracy for DP models while retaining privacy guarantees.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Hsiang Hsu, Shahab Asoodeh and Flavio Calmon
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs36" class="paper-title">
                        Discovering Information-Leaking Samples and Features
                        </a> &nbsp;&nbsp;
                        <!-- <a href="https://arxiv.org" class="link-paper">[arxiv]</a> -->
                    </div>
                    <!-- <div id="abs36" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div> -->
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Martine De Cock, Rafael Dowsley, Anderson Nascimento, Davis Railsback, Jianwei Shen and Ariel Todoki
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs37" class="paper-title">
                        Fast Secure Logistic Regression for High Dimensional Gene Data
                        </a> &nbsp;&nbsp;
                        <!-- <a href="https://arxiv.org" class="link-paper">[arxiv]</a> -->
                    </div>
                    <!-- <div id="abs37" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div> -->
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Giuseppe Vietri, Grace Tian, Mark Bun, Thomas Steinke and Steven Wu
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs38" class="paper-title">
                        New Oracle-Efficient Algorithms for Private Synthetic Data Release
                        </a> &nbsp;&nbsp;
                        <!-- <a href="https://arxiv.org" class="link-paper">[arxiv]</a> -->
                    </div>
                    <!-- <div id="abs38" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div> -->
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Abraham Flaxman
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs39" class="paper-title">
                        Empirical quantification of privacy loss with examples relevant to the 2020 US Census
                        </a> &nbsp;&nbsp;
                        <!-- <a href="https://arxiv.org" class="link-paper">[arxiv]</a> -->
                    </div>
                    <!-- <div id="abs39" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div> -->
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Shadi Rahimian, Tribhuvanesh Orekondy and Mario Fritz
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs40" class="paper-title">
                        Differential Privacy Defenses and Sampling Attacks for Membership Inference
                        </a> &nbsp;&nbsp;
                        <!-- <a href="https://arxiv.org" class="link-paper">[arxiv]</a> -->
                    </div>
                    <!-- <div id="abs40" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div> -->
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Julius Adebayo, Hal Abelson and Danny Weitzner
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs41" class="paper-title">
                        Tensions Between Differential Privacy and Local Explanations for Deep Neural Networks
                        </a> &nbsp;&nbsp;
                        <a href="https://www.dropbox.com/s/exeul9eprt3as5m/dp_explanations_neurips_workshop.pdf?dl=0" class="link-paper">[pdf]</a>
                    </div>
                    <div id="abs41" class="panel-footer panel-paper-footer collapse">
                    As deep learning models are beginning to be applied in high stakes and consequential settings,
                    there has been increased interest in interpretability methods for gaining insight into these models.
                    Similarly, privacy is also of paramount importance in these settings.
                    Here, we examine tradeoffs that occur between feature attribution maps,
                    an increasingly popular medium to provide interpretations, and model privacy.
                    Attribution maps reveal information about the features of the input that have the most relevance to the output of a model.
                    On the other hand, differential privacy (DP) seeks to protect against learning information about individual inputs.
                    Consequently, both requirements seem to be at odds.
                    More distressingly, Shokri et al.[1] recently showed that one can infer training set membership using attribution maps.
                    Towards this end, we consider attribution maps derived from models trained to be differentially private.
                    Empirically, we find that there is a trade-off between privacy and interpretability.
                    Attribution maps that seek to reconstruct the input are not sensitive to the privacy level of the model,
                    and consequently reveal the input.
                    On the other hand, attributions that show sensitivity to model privacy degrade sharply in visual coherence.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Amos Beimel, Aleksandra Korolova, Kobbi Nissim, Or Sheffet and Uri Stemmer
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs42" class="paper-title">
                        The Power of Synergy in Differential Privacy: Combining a Small Curator with Local Randomizers
                        </a> &nbsp;&nbsp;
                        <!-- <a href="https://arxiv.org" class="link-paper">[arxiv]</a> -->
                    </div>
                    <!-- <div id="abs42" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div> -->
                </div>
        </div>
        </div>
    </section>


    <!-- Call for travel grants -->
    <!--
        <section id="grants" class="container content-section text-center">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2">
                    <h2>Travel Grants</h2>
                    <p>
                    Thanks to our generous sponsors, we are able to provide a limited number of travel grants of up to $800 to help partially cover the expenses of authors of accepted papers who have not received other travel support from NeurIPS this year.
                    To apply, please send an email to <a href="mailto:ppml18@easychair.org?Subject=PPML18%20Travel%20Grant%20Application">ppml18@easychair.org</a> with the subject “PPML18 Travel Grant Application” including your resume and a half-page statement of purpose mentioning the title and the authors of your accepted paper and a summary of anticipated travel expenses. If you are an undergraduate or graduate student, we ask for a half-page recommendation letter supporting your application to be sent to us by the deadline. The deadline for applications is <b>November 11, 2018 (11:59pm AoE)</b>. The notifications will be sent by <b>November 16</b>. Please feel free to send us an email if you have any questions.
                </div>
            </div>
        </section>
    -->

    <!-- Organizers Section -->
    <section id="organizers" class="content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Organization</h2>
                <br />
                <h3>Workshop organizers</h3>
                <ul class="list-group">
                    <li class="list-group-item organizer">Borja Balle (DeepMind)</li>
                    <li class="list-group-item organizer">Kamalika Chaudhuri (UC San Diego)</li>
                    <li class="list-group-item organizer">Antti Honkela (University of Helsinki)</li>
                    <li class="list-group-item organizer">Antti Koskela (University of Helsinki)</li>
                    <li class="list-group-item organizer">Casey Meehan (UC San Diego)</li>
                    <li class="list-group-item organizer">Mijung Park (Max Planck Institute for Intelligent Systems)</li>
                    <li class="list-group-item organizer">Mary Anne Smart (UC San Diego)</li>
                    <li class="list-group-item organizer">Adrian Weller (Alan Turing Institute & Cambridge)</li>
                </ul>
                <br />
                <h3>Program Committee</h3>
                <ul class="list-group">
                  <li class="list-group-item organizer">James Bell (University of Cambridge)</li>
                  <li class="list-group-item organizer">Aurélien Bellet (INRIA)</li>
                  <li class="list-group-item organizer">Mark Bun (Boston University)</li>
                  <li class="list-group-item organizer">Christos Dimitrakakis (Chalmers University / University of Lille / Harvard University)</li>
                  <li class="list-group-item organizer">James Foulds (University of Maryland, Baltimore County)</li>
                  <li class="list-group-item organizer">Matt Fredrikson (Carnegie Mellon University)</li>
                  <li class="list-group-item organizer">Marco Gaboardi (University at Buffalo, SUNY)</li>
                  <li class="list-group-item organizer">Adria Gascon (The Alan Turing Institute / Warwick University)</li>
                  <li class="list-group-item organizer">Alon Gonen (Princeton University)</li>
                  <li class="list-group-item organizer">Peter Kairouz (Google AI)</li>
                  <li class="list-group-item organizer">Gautam Kamath (University of Waterloo)</li>
                  <li class="list-group-item organizer">Marcel Keller (Data61)</li>
                  <li class="list-group-item organizer">Nadin Kokciyan (King's College London)</li>
                  <li class="list-group-item organizer">Aleksandra Korolova (University of Southern California)</li>
                  <li class="list-group-item organizer">Audra McMillan (Boston University and Northeastern University)</li>
                  <li class="list-group-item organizer">Olga Ohrimenko (Microsoft)</li>
                  <li class="list-group-item organizer">Jun Sakuma (University of Tsukuba)</li>
                  <li class="list-group-item organizer">Anand Sarwate (Rutgers University)</li>
                  <li class="list-group-item organizer">Phillipp Schoppmann (Humboldt University of Berlin)</li>
                  <li class="list-group-item organizer">Or Sheffet (University of Alberta)</li>
                  <li class="list-group-item organizer">Kana Shimizu (Computational Biology Research Center, AIST)</li>
                  <li class="list-group-item organizer">Thomas Steinke (IBM)</li>
                  <li class="list-group-item organizer">Kunal Talwar (Google)</li>
                  <li class="list-group-item organizer">Carmela Troncoso (Ecole Polytechnique Fédérale de Lausanne)</li>
                  <li class="list-group-item organizer">Yu-Xiang Wang (Carnegie Mellon University)</li>
                  <li class="list-group-item organizer"></li>
                </ul>
                <!-- <br />
                <h3>Sponsors</h3>
                <br />
                    <img style="margin:50px;"height="80" src="img/ati-white.png"> -->
                    <!-- <img style="margin:50px;"height="80" src="img/amazon.png">
                    <img style="margin:50px;"height="80" src="img/google.png">
                    <img style="margin:50px;"height="80" src="img/microsoft.png"> -->
            </div>
        </div>
    </section>

    <!-- Accessibility Section -->
    <section id="access" class="content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Accessibility</h2>
                <br />
                <p>
                  By taking a few simple steps&mdash;such as paying special attention to font sizes and captions&mdash;
                  you can make your
                  <a href="https://www.washington.edu/doit/how-can-you-make-your-presentation-accessible">presentations</a> and
                  <a href="resources/accessibility_posters_gilson2019.pdf">posters</a>
                  more accessible.
                  Feel free to <a href="mailto:priml2019@easychair.org">contact us</a> about any accessibility concerns relating to the website, workshop, etc.
                </p>
                <br />
                <br />
                <br />
                <br />
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer>
        <div class="container text-center" >
          <p>Sponsored by</p>
            <a href="https://www.turing.ac.uk/"><img height="50" style="margin:30px;" src="img/ati-white.png"></a>
            <a class="hide-on-mobile" href="http://lcfi.ac.uk/"><img height="50" style="margin:20px;" src="img/cfi.jpg"></a>
            <a class="mobile-only" href="http://lcfi.ac.uk/"><img width="275" style="margin:10px;" src="img/cfi.jpg"></a>
            <a href="https://deepmind.com/"><img height="50" style="margin:30px;" src="img/DM.png"></a>
        </div>
        <div class="container text-center">
            <p>Contact us: <a href="mailto:priml2019@easychair.org">priml2019@easychair.org</a></p>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="js/jquery.min.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>

    <!-- Theme JavaScript -->
    <script src="js/script.js"></script>

</body>

</html>
